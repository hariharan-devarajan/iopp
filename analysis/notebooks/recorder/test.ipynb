{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyze HACC IO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Global Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "notebook_name = \"hacc\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## System Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import socket\n",
    "import os\n",
    "import time\n",
    "from time import sleep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Create Dask Cluster for analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dask Job Queue Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/workspace/iopp/.conda/envs/jupyter/lib/python3.9/site-packages/dask_jobqueue/core.py:17: FutureWarning: format_bytes is deprecated and will be removed in a future release. Please use dask.utils.format_bytes instead.\n",
      "  from distributed.utils import format_bytes, parse_bytes, tmpfile, get_ip_interface\n",
      "/usr/workspace/iopp/.conda/envs/jupyter/lib/python3.9/site-packages/dask_jobqueue/core.py:17: FutureWarning: parse_bytes is deprecated and will be removed in a future release. Please use dask.utils.parse_bytes instead.\n",
      "  from distributed.utils import format_bytes, parse_bytes, tmpfile, get_ip_interface\n",
      "/usr/workspace/iopp/.conda/envs/jupyter/lib/python3.9/site-packages/dask_jobqueue/htcondor.py:6: FutureWarning: parse_bytes is deprecated and will be removed in a future release. Please use dask.utils.parse_bytes instead.\n",
      "  from distributed.utils import parse_bytes\n"
     ]
    }
   ],
   "source": [
    "import dask_jobqueue\n",
    "from dask_jobqueue import LSFCluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dask Worker configuration\n",
    "When we use dask for analysis, we need to consider the type of analysis. In this case we plan to utilize dask dataframes for analysis that is typically memory intensive. Therefore, we allocate the whole memory per node and use only 4 worker processes per node. More worker processes reduce memory available per worker resulting in frequent memory swap from filesystem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-16T21:37:00.088821Z",
     "start_time": "2021-09-16T21:37:00.076921Z"
    }
   },
   "outputs": [],
   "source": [
    "node_memory = 1600 # node memory in GB\n",
    "n_workers_per_node = 16 # number of worker processes per node\n",
    "n_threads_per_worker = 1\n",
    "worker_time = 120 # job time per node for worker\n",
    "worker_queue = \"pdebug\" # queue to be used per worker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Other configurations we can compute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import socket\n",
    "host = socket.gethostname()\n",
    "dashboard_address = '{}:8788'.format(socket.gethostname())\n",
    "memory = '{}GB'.format(node_memory/n_workers_per_node)\n",
    "job_extra = ['-nnodes 1', \n",
    "             '-G asccasc', \n",
    "             '-q {}'.format(worker_queue), \n",
    "             '-W {}'.format(worker_time), \n",
    "             '-o {}.log'.format(notebook_name), \n",
    "             '-e {}.log'.format(notebook_name)]\n",
    "local_directory=\"/p/gpfs1/iopp/temp/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Dask Cluster "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.dataframe as dd\n",
    "from dask.distributed import wait, progress\n",
    "from dask.diagnostics import ProgressBar\n",
    "from dask.distributed import Client, LocalCluster\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import concurrent.futures\n",
    "from concurrent.futures import ThreadPoolExecutor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_groups_indices = ['tmid', 'proc_id', 'file_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-16T21:37:00.580186Z",
     "start_time": "2021-09-16T21:37:00.574875Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'cls': <class 'distributed.scheduler.Scheduler'>, 'options': {'protocol': 'tcp://', 'interface': None, 'host': 'lassen397', 'dashboard_address': 'lassen397:8788', 'security': None}}\n",
      "{'cls': <class 'distributed.scheduler.Scheduler'>, 'options': {'protocol': 'tcp://', 'interface': None, 'host': 'lassen397', 'dashboard_address': 'lassen397:8789', 'security': None}}\n",
      "{'cls': <class 'distributed.scheduler.Scheduler'>, 'options': {'protocol': 'tcp://', 'interface': None, 'host': 'lassen397', 'dashboard_address': 'lassen397:8790', 'security': None}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/iopp/temp/local/dask-worker-space/worker-z2lxiyv8', purging\n",
      "distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/iopp/temp/local/dask-worker-space/worker-60gmu9ta', purging\n",
      "distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/iopp/temp/local/dask-worker-space/worker-n7d_sk_u', purging\n",
      "distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/iopp/temp/local/dask-worker-space/worker-kijhqzlz', purging\n",
      "distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/iopp/temp/local/dask-worker-space/worker-hc6qbfr9', purging\n",
      "distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/iopp/temp/local/dask-worker-space/worker-qzdrtoir', purging\n",
      "distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/iopp/temp/local/dask-worker-space/worker-foceq5ad', purging\n",
      "distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/iopp/temp/local/dask-worker-space/worker-uj8fzsib', purging\n"
     ]
    }
   ],
   "source": [
    "clusters = {}\n",
    "for i, index in enumerate(filter_groups_indices):\n",
    "    clusters[index] = LSFCluster(cores = n_workers_per_node*n_threads_per_worker,processes=n_workers_per_node, memory='{}GB'.format(node_memory), \n",
    "                     header_skip=['-n ','-R','-M', '-P', '-W 00:30'], \n",
    "                     job_extra = job_extra, \n",
    "                     death_timeout=worker_time*60,\n",
    "                     local_directory =f\"{local_directory}/{index}\",\n",
    "                     use_stdin=True, host = host,dashboard_address = f'{socket.gethostname()}:{8788+i}')\n",
    "local_cluster = LocalCluster(n_workers=8,local_directory =f\"{local_directory}/local\", dashboard_address= f':8797', memory_limit=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dask Analysis Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# workers = n_workers_per_node\n",
    "# timeline_cluster = LocalCluster(n_workers=workers,local_directory=f\"{local_directory}/timeline\")\n",
    "# file_cluster = LocalCluster(n_workers=workers,local_directory =f\"{local_directory}/file\")\n",
    "# local_cluster = LocalCluster(n_workers=workers,local_directory =f\"{local_directory}/local\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import os\n",
    "#hosts_env= os.environ.get(\"LLNL_COMPUTE_NODES\", \"localhost[]\")\n",
    "#print(hosts_env)\n",
    "#import re\n",
    "#hosts = re.split('\\[|\\]|,', hosts_env)[1:-1]\n",
    "#hosts = [\"lassen\" +s for s in hosts]\n",
    "#print(hosts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "##hostname = hosts[0]\n",
    "#print(hostname)\n",
    "#!ssh ${hostname} \"pid=\\$(ps aux | grep 'dask' | awk '{print \\$2}' | head -1); echo \\$pid |xargs kill\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from dask.distributed import Client, SSHCluster\n",
    "# timeline_cluster = SSHCluster(\n",
    "#     [hosts[0]],\n",
    "#     connect_options={\"known_hosts\": None},\n",
    "#     worker_options={\"nthreads\": n_threads_per_worker, \"n_workers\": n_workers_per_node,  \"local_directory\" :f\"{local_directory}/timeline\"},\n",
    "#     scheduler_options={\"port\": 0, \"dashboard_address\": ':8799'}\n",
    "# )\n",
    "# file_cluster = SSHCluster(\n",
    "#     [hosts[1]],\n",
    "#     connect_options={\"known_hosts\": None},\n",
    "#     worker_options={\"nthreads\": n_threads_per_worker, \"n_workers\": n_workers_per_node,  \"local_directory\" :f\"{local_directory}/file\"},\n",
    "#     scheduler_options={\"port\": 0, \"dashboard_address\": ':8798'}\n",
    "   \n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cluster = SSHCluster([hosts[0]],\n",
    "#     connect_options={\"known_hosts\": None})\n",
    "# client = Client(cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cluster.scale(n_workers_per_node)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Dask Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "clients = {}\n",
    "for index in filter_groups_indices:\n",
    "    clients[index] = Client(clusters[index], set_as_default=False)\n",
    "local_client = Client(local_cluster)\n",
    "#timeline_client = \n",
    "#file_client = Client(file_cluster, set_as_default=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#timeline_cluster.scale(n_workers)\n",
    "\n",
    "for index in filter_groups_indices:\n",
    "    clusters[index].scale(n_workers_per_node * 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Client: 'tcp://192.168.65.143:45453' processes=0 threads=0, memory=0 B>\n",
      "<Client: 'tcp://192.168.65.143:35837' processes=0 threads=0, memory=0 B>\n",
      "<Client: 'tcp://192.168.65.143:36401' processes=0 threads=0, memory=0 B>\n"
     ]
    }
   ],
   "source": [
    "for index in filter_groups_indices:\n",
    "    print(clients[index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "    <div style=\"width: 24px; height: 24px; background-color: #e1e1e1; border: 3px solid #9D9D9D; border-radius: 5px; position: absolute;\"> </div>\n",
       "    <div style=\"margin-left: 48px;\">\n",
       "        <h3 style=\"margin-bottom: 0px;\">Client</h3>\n",
       "        <p style=\"color: #9D9D9D; margin-bottom: 0px;\">Client-5e8bc068-3264-11ed-969c-70e28414eb5d</p>\n",
       "        <table style=\"width: 100%; text-align: left;\">\n",
       "\n",
       "        <tr>\n",
       "        \n",
       "            <td style=\"text-align: left;\"><strong>Connection method:</strong> Cluster object</td>\n",
       "            <td style=\"text-align: left;\"><strong>Cluster type:</strong> distributed.LocalCluster</td>\n",
       "        \n",
       "        </tr>\n",
       "\n",
       "        \n",
       "            <tr>\n",
       "                <td style=\"text-align: left;\">\n",
       "                    <strong>Dashboard: </strong> <a href=\"http://127.0.0.1:8797/status\" target=\"_blank\">http://127.0.0.1:8797/status</a>\n",
       "                </td>\n",
       "                <td style=\"text-align: left;\"></td>\n",
       "            </tr>\n",
       "        \n",
       "\n",
       "        </table>\n",
       "\n",
       "        \n",
       "            <details>\n",
       "            <summary style=\"margin-bottom: 20px;\"><h3 style=\"display: inline;\">Cluster Info</h3></summary>\n",
       "            <div class=\"jp-RenderedHTMLCommon jp-RenderedHTML jp-mod-trusted jp-OutputArea-output\">\n",
       "    <div style=\"width: 24px; height: 24px; background-color: #e1e1e1; border: 3px solid #9D9D9D; border-radius: 5px; position: absolute;\">\n",
       "    </div>\n",
       "    <div style=\"margin-left: 48px;\">\n",
       "        <h3 style=\"margin-bottom: 0px; margin-top: 0px;\">LocalCluster</h3>\n",
       "        <p style=\"color: #9D9D9D; margin-bottom: 0px;\">3a590430</p>\n",
       "        <table style=\"width: 100%; text-align: left;\">\n",
       "            <tr>\n",
       "                <td style=\"text-align: left;\">\n",
       "                    <strong>Dashboard:</strong> <a href=\"http://127.0.0.1:8797/status\" target=\"_blank\">http://127.0.0.1:8797/status</a>\n",
       "                </td>\n",
       "                <td style=\"text-align: left;\">\n",
       "                    <strong>Workers:</strong> 8\n",
       "                </td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                <td style=\"text-align: left;\">\n",
       "                    <strong>Total threads:</strong> 160\n",
       "                </td>\n",
       "                <td style=\"text-align: left;\">\n",
       "                    <strong>Total memory:</strong> 0 B\n",
       "                </td>\n",
       "            </tr>\n",
       "            \n",
       "            <tr>\n",
       "    <td style=\"text-align: left;\"><strong>Status:</strong> running</td>\n",
       "    <td style=\"text-align: left;\"><strong>Using processes:</strong> True</td>\n",
       "</tr>\n",
       "\n",
       "            \n",
       "        </table>\n",
       "\n",
       "        <details>\n",
       "            <summary style=\"margin-bottom: 20px;\">\n",
       "                <h3 style=\"display: inline;\">Scheduler Info</h3>\n",
       "            </summary>\n",
       "\n",
       "            <div style=\"\">\n",
       "    <div>\n",
       "        <div style=\"width: 24px; height: 24px; background-color: #FFF7E5; border: 3px solid #FF6132; border-radius: 5px; position: absolute;\"> </div>\n",
       "        <div style=\"margin-left: 48px;\">\n",
       "            <h3 style=\"margin-bottom: 0px;\">Scheduler</h3>\n",
       "            <p style=\"color: #9D9D9D; margin-bottom: 0px;\">Scheduler-8eb97e08-932a-434f-82e6-f3ea75a7cfd7</p>\n",
       "            <table style=\"width: 100%; text-align: left;\">\n",
       "                <tr>\n",
       "                    <td style=\"text-align: left;\">\n",
       "                        <strong>Comm:</strong> tcp://127.0.0.1:46087\n",
       "                    </td>\n",
       "                    <td style=\"text-align: left;\">\n",
       "                        <strong>Workers:</strong> 8\n",
       "                    </td>\n",
       "                </tr>\n",
       "                <tr>\n",
       "                    <td style=\"text-align: left;\">\n",
       "                        <strong>Dashboard:</strong> <a href=\"http://127.0.0.1:8797/status\" target=\"_blank\">http://127.0.0.1:8797/status</a>\n",
       "                    </td>\n",
       "                    <td style=\"text-align: left;\">\n",
       "                        <strong>Total threads:</strong> 160\n",
       "                    </td>\n",
       "                </tr>\n",
       "                <tr>\n",
       "                    <td style=\"text-align: left;\">\n",
       "                        <strong>Started:</strong> Just now\n",
       "                    </td>\n",
       "                    <td style=\"text-align: left;\">\n",
       "                        <strong>Total memory:</strong> 0 B\n",
       "                    </td>\n",
       "                </tr>\n",
       "            </table>\n",
       "        </div>\n",
       "    </div>\n",
       "\n",
       "    <details style=\"margin-left: 48px;\">\n",
       "        <summary style=\"margin-bottom: 20px;\">\n",
       "            <h3 style=\"display: inline;\">Workers</h3>\n",
       "        </summary>\n",
       "\n",
       "        \n",
       "        <div style=\"margin-bottom: 20px;\">\n",
       "            <div style=\"width: 24px; height: 24px; background-color: #DBF5FF; border: 3px solid #4CC9FF; border-radius: 5px; position: absolute;\"> </div>\n",
       "            <div style=\"margin-left: 48px;\">\n",
       "            <details>\n",
       "                <summary>\n",
       "                    <h4 style=\"margin-bottom: 0px; display: inline;\">Worker: 0</h4>\n",
       "                </summary>\n",
       "                <table style=\"width: 100%; text-align: left;\">\n",
       "                    <tr>\n",
       "                        <td style=\"text-align: left;\">\n",
       "                            <strong>Comm: </strong> tcp://127.0.0.1:43987\n",
       "                        </td>\n",
       "                        <td style=\"text-align: left;\">\n",
       "                            <strong>Total threads: </strong> 20\n",
       "                        </td>\n",
       "                    </tr>\n",
       "                    <tr>\n",
       "                        <td style=\"text-align: left;\">\n",
       "                            <strong>Dashboard: </strong> <a href=\"http://127.0.0.1:39997/status\" target=\"_blank\">http://127.0.0.1:39997/status</a>\n",
       "                        </td>\n",
       "                        <td style=\"text-align: left;\">\n",
       "                            <strong>Memory: </strong> 0 B\n",
       "                        </td>\n",
       "                    </tr>\n",
       "                    <tr>\n",
       "                        <td style=\"text-align: left;\">\n",
       "                            <strong>Nanny: </strong> tcp://127.0.0.1:41707\n",
       "                        </td>\n",
       "                        <td style=\"text-align: left;\"></td>\n",
       "                    </tr>\n",
       "                    <tr>\n",
       "                        <td colspan=\"2\" style=\"text-align: left;\">\n",
       "                            <strong>Local directory: </strong> /p/gpfs1/iopp/temp/local/dask-worker-space/worker-40iba3bi\n",
       "                        </td>\n",
       "                    </tr>\n",
       "\n",
       "                    \n",
       "\n",
       "                    \n",
       "\n",
       "                </table>\n",
       "            </details>\n",
       "            </div>\n",
       "        </div>\n",
       "        \n",
       "        <div style=\"margin-bottom: 20px;\">\n",
       "            <div style=\"width: 24px; height: 24px; background-color: #DBF5FF; border: 3px solid #4CC9FF; border-radius: 5px; position: absolute;\"> </div>\n",
       "            <div style=\"margin-left: 48px;\">\n",
       "            <details>\n",
       "                <summary>\n",
       "                    <h4 style=\"margin-bottom: 0px; display: inline;\">Worker: 1</h4>\n",
       "                </summary>\n",
       "                <table style=\"width: 100%; text-align: left;\">\n",
       "                    <tr>\n",
       "                        <td style=\"text-align: left;\">\n",
       "                            <strong>Comm: </strong> tcp://127.0.0.1:43377\n",
       "                        </td>\n",
       "                        <td style=\"text-align: left;\">\n",
       "                            <strong>Total threads: </strong> 20\n",
       "                        </td>\n",
       "                    </tr>\n",
       "                    <tr>\n",
       "                        <td style=\"text-align: left;\">\n",
       "                            <strong>Dashboard: </strong> <a href=\"http://127.0.0.1:34521/status\" target=\"_blank\">http://127.0.0.1:34521/status</a>\n",
       "                        </td>\n",
       "                        <td style=\"text-align: left;\">\n",
       "                            <strong>Memory: </strong> 0 B\n",
       "                        </td>\n",
       "                    </tr>\n",
       "                    <tr>\n",
       "                        <td style=\"text-align: left;\">\n",
       "                            <strong>Nanny: </strong> tcp://127.0.0.1:43717\n",
       "                        </td>\n",
       "                        <td style=\"text-align: left;\"></td>\n",
       "                    </tr>\n",
       "                    <tr>\n",
       "                        <td colspan=\"2\" style=\"text-align: left;\">\n",
       "                            <strong>Local directory: </strong> /p/gpfs1/iopp/temp/local/dask-worker-space/worker-foelktsz\n",
       "                        </td>\n",
       "                    </tr>\n",
       "\n",
       "                    \n",
       "\n",
       "                    \n",
       "\n",
       "                </table>\n",
       "            </details>\n",
       "            </div>\n",
       "        </div>\n",
       "        \n",
       "        <div style=\"margin-bottom: 20px;\">\n",
       "            <div style=\"width: 24px; height: 24px; background-color: #DBF5FF; border: 3px solid #4CC9FF; border-radius: 5px; position: absolute;\"> </div>\n",
       "            <div style=\"margin-left: 48px;\">\n",
       "            <details>\n",
       "                <summary>\n",
       "                    <h4 style=\"margin-bottom: 0px; display: inline;\">Worker: 2</h4>\n",
       "                </summary>\n",
       "                <table style=\"width: 100%; text-align: left;\">\n",
       "                    <tr>\n",
       "                        <td style=\"text-align: left;\">\n",
       "                            <strong>Comm: </strong> tcp://127.0.0.1:40069\n",
       "                        </td>\n",
       "                        <td style=\"text-align: left;\">\n",
       "                            <strong>Total threads: </strong> 20\n",
       "                        </td>\n",
       "                    </tr>\n",
       "                    <tr>\n",
       "                        <td style=\"text-align: left;\">\n",
       "                            <strong>Dashboard: </strong> <a href=\"http://127.0.0.1:39719/status\" target=\"_blank\">http://127.0.0.1:39719/status</a>\n",
       "                        </td>\n",
       "                        <td style=\"text-align: left;\">\n",
       "                            <strong>Memory: </strong> 0 B\n",
       "                        </td>\n",
       "                    </tr>\n",
       "                    <tr>\n",
       "                        <td style=\"text-align: left;\">\n",
       "                            <strong>Nanny: </strong> tcp://127.0.0.1:43805\n",
       "                        </td>\n",
       "                        <td style=\"text-align: left;\"></td>\n",
       "                    </tr>\n",
       "                    <tr>\n",
       "                        <td colspan=\"2\" style=\"text-align: left;\">\n",
       "                            <strong>Local directory: </strong> /p/gpfs1/iopp/temp/local/dask-worker-space/worker-9icd84ry\n",
       "                        </td>\n",
       "                    </tr>\n",
       "\n",
       "                    \n",
       "\n",
       "                    \n",
       "\n",
       "                </table>\n",
       "            </details>\n",
       "            </div>\n",
       "        </div>\n",
       "        \n",
       "        <div style=\"margin-bottom: 20px;\">\n",
       "            <div style=\"width: 24px; height: 24px; background-color: #DBF5FF; border: 3px solid #4CC9FF; border-radius: 5px; position: absolute;\"> </div>\n",
       "            <div style=\"margin-left: 48px;\">\n",
       "            <details>\n",
       "                <summary>\n",
       "                    <h4 style=\"margin-bottom: 0px; display: inline;\">Worker: 3</h4>\n",
       "                </summary>\n",
       "                <table style=\"width: 100%; text-align: left;\">\n",
       "                    <tr>\n",
       "                        <td style=\"text-align: left;\">\n",
       "                            <strong>Comm: </strong> tcp://127.0.0.1:36701\n",
       "                        </td>\n",
       "                        <td style=\"text-align: left;\">\n",
       "                            <strong>Total threads: </strong> 20\n",
       "                        </td>\n",
       "                    </tr>\n",
       "                    <tr>\n",
       "                        <td style=\"text-align: left;\">\n",
       "                            <strong>Dashboard: </strong> <a href=\"http://127.0.0.1:42717/status\" target=\"_blank\">http://127.0.0.1:42717/status</a>\n",
       "                        </td>\n",
       "                        <td style=\"text-align: left;\">\n",
       "                            <strong>Memory: </strong> 0 B\n",
       "                        </td>\n",
       "                    </tr>\n",
       "                    <tr>\n",
       "                        <td style=\"text-align: left;\">\n",
       "                            <strong>Nanny: </strong> tcp://127.0.0.1:45633\n",
       "                        </td>\n",
       "                        <td style=\"text-align: left;\"></td>\n",
       "                    </tr>\n",
       "                    <tr>\n",
       "                        <td colspan=\"2\" style=\"text-align: left;\">\n",
       "                            <strong>Local directory: </strong> /p/gpfs1/iopp/temp/local/dask-worker-space/worker-6_ubay4a\n",
       "                        </td>\n",
       "                    </tr>\n",
       "\n",
       "                    \n",
       "\n",
       "                    \n",
       "\n",
       "                </table>\n",
       "            </details>\n",
       "            </div>\n",
       "        </div>\n",
       "        \n",
       "        <div style=\"margin-bottom: 20px;\">\n",
       "            <div style=\"width: 24px; height: 24px; background-color: #DBF5FF; border: 3px solid #4CC9FF; border-radius: 5px; position: absolute;\"> </div>\n",
       "            <div style=\"margin-left: 48px;\">\n",
       "            <details>\n",
       "                <summary>\n",
       "                    <h4 style=\"margin-bottom: 0px; display: inline;\">Worker: 4</h4>\n",
       "                </summary>\n",
       "                <table style=\"width: 100%; text-align: left;\">\n",
       "                    <tr>\n",
       "                        <td style=\"text-align: left;\">\n",
       "                            <strong>Comm: </strong> tcp://127.0.0.1:44655\n",
       "                        </td>\n",
       "                        <td style=\"text-align: left;\">\n",
       "                            <strong>Total threads: </strong> 20\n",
       "                        </td>\n",
       "                    </tr>\n",
       "                    <tr>\n",
       "                        <td style=\"text-align: left;\">\n",
       "                            <strong>Dashboard: </strong> <a href=\"http://127.0.0.1:39753/status\" target=\"_blank\">http://127.0.0.1:39753/status</a>\n",
       "                        </td>\n",
       "                        <td style=\"text-align: left;\">\n",
       "                            <strong>Memory: </strong> 0 B\n",
       "                        </td>\n",
       "                    </tr>\n",
       "                    <tr>\n",
       "                        <td style=\"text-align: left;\">\n",
       "                            <strong>Nanny: </strong> tcp://127.0.0.1:35653\n",
       "                        </td>\n",
       "                        <td style=\"text-align: left;\"></td>\n",
       "                    </tr>\n",
       "                    <tr>\n",
       "                        <td colspan=\"2\" style=\"text-align: left;\">\n",
       "                            <strong>Local directory: </strong> /p/gpfs1/iopp/temp/local/dask-worker-space/worker-ru5i102o\n",
       "                        </td>\n",
       "                    </tr>\n",
       "\n",
       "                    \n",
       "\n",
       "                    \n",
       "\n",
       "                </table>\n",
       "            </details>\n",
       "            </div>\n",
       "        </div>\n",
       "        \n",
       "        <div style=\"margin-bottom: 20px;\">\n",
       "            <div style=\"width: 24px; height: 24px; background-color: #DBF5FF; border: 3px solid #4CC9FF; border-radius: 5px; position: absolute;\"> </div>\n",
       "            <div style=\"margin-left: 48px;\">\n",
       "            <details>\n",
       "                <summary>\n",
       "                    <h4 style=\"margin-bottom: 0px; display: inline;\">Worker: 5</h4>\n",
       "                </summary>\n",
       "                <table style=\"width: 100%; text-align: left;\">\n",
       "                    <tr>\n",
       "                        <td style=\"text-align: left;\">\n",
       "                            <strong>Comm: </strong> tcp://127.0.0.1:38877\n",
       "                        </td>\n",
       "                        <td style=\"text-align: left;\">\n",
       "                            <strong>Total threads: </strong> 20\n",
       "                        </td>\n",
       "                    </tr>\n",
       "                    <tr>\n",
       "                        <td style=\"text-align: left;\">\n",
       "                            <strong>Dashboard: </strong> <a href=\"http://127.0.0.1:46131/status\" target=\"_blank\">http://127.0.0.1:46131/status</a>\n",
       "                        </td>\n",
       "                        <td style=\"text-align: left;\">\n",
       "                            <strong>Memory: </strong> 0 B\n",
       "                        </td>\n",
       "                    </tr>\n",
       "                    <tr>\n",
       "                        <td style=\"text-align: left;\">\n",
       "                            <strong>Nanny: </strong> tcp://127.0.0.1:42793\n",
       "                        </td>\n",
       "                        <td style=\"text-align: left;\"></td>\n",
       "                    </tr>\n",
       "                    <tr>\n",
       "                        <td colspan=\"2\" style=\"text-align: left;\">\n",
       "                            <strong>Local directory: </strong> /p/gpfs1/iopp/temp/local/dask-worker-space/worker-txjlxetn\n",
       "                        </td>\n",
       "                    </tr>\n",
       "\n",
       "                    \n",
       "\n",
       "                    \n",
       "\n",
       "                </table>\n",
       "            </details>\n",
       "            </div>\n",
       "        </div>\n",
       "        \n",
       "        <div style=\"margin-bottom: 20px;\">\n",
       "            <div style=\"width: 24px; height: 24px; background-color: #DBF5FF; border: 3px solid #4CC9FF; border-radius: 5px; position: absolute;\"> </div>\n",
       "            <div style=\"margin-left: 48px;\">\n",
       "            <details>\n",
       "                <summary>\n",
       "                    <h4 style=\"margin-bottom: 0px; display: inline;\">Worker: 6</h4>\n",
       "                </summary>\n",
       "                <table style=\"width: 100%; text-align: left;\">\n",
       "                    <tr>\n",
       "                        <td style=\"text-align: left;\">\n",
       "                            <strong>Comm: </strong> tcp://127.0.0.1:34301\n",
       "                        </td>\n",
       "                        <td style=\"text-align: left;\">\n",
       "                            <strong>Total threads: </strong> 20\n",
       "                        </td>\n",
       "                    </tr>\n",
       "                    <tr>\n",
       "                        <td style=\"text-align: left;\">\n",
       "                            <strong>Dashboard: </strong> <a href=\"http://127.0.0.1:44159/status\" target=\"_blank\">http://127.0.0.1:44159/status</a>\n",
       "                        </td>\n",
       "                        <td style=\"text-align: left;\">\n",
       "                            <strong>Memory: </strong> 0 B\n",
       "                        </td>\n",
       "                    </tr>\n",
       "                    <tr>\n",
       "                        <td style=\"text-align: left;\">\n",
       "                            <strong>Nanny: </strong> tcp://127.0.0.1:38151\n",
       "                        </td>\n",
       "                        <td style=\"text-align: left;\"></td>\n",
       "                    </tr>\n",
       "                    <tr>\n",
       "                        <td colspan=\"2\" style=\"text-align: left;\">\n",
       "                            <strong>Local directory: </strong> /p/gpfs1/iopp/temp/local/dask-worker-space/worker-rnw4mwzs\n",
       "                        </td>\n",
       "                    </tr>\n",
       "\n",
       "                    \n",
       "\n",
       "                    \n",
       "\n",
       "                </table>\n",
       "            </details>\n",
       "            </div>\n",
       "        </div>\n",
       "        \n",
       "        <div style=\"margin-bottom: 20px;\">\n",
       "            <div style=\"width: 24px; height: 24px; background-color: #DBF5FF; border: 3px solid #4CC9FF; border-radius: 5px; position: absolute;\"> </div>\n",
       "            <div style=\"margin-left: 48px;\">\n",
       "            <details>\n",
       "                <summary>\n",
       "                    <h4 style=\"margin-bottom: 0px; display: inline;\">Worker: 7</h4>\n",
       "                </summary>\n",
       "                <table style=\"width: 100%; text-align: left;\">\n",
       "                    <tr>\n",
       "                        <td style=\"text-align: left;\">\n",
       "                            <strong>Comm: </strong> tcp://127.0.0.1:36281\n",
       "                        </td>\n",
       "                        <td style=\"text-align: left;\">\n",
       "                            <strong>Total threads: </strong> 20\n",
       "                        </td>\n",
       "                    </tr>\n",
       "                    <tr>\n",
       "                        <td style=\"text-align: left;\">\n",
       "                            <strong>Dashboard: </strong> <a href=\"http://127.0.0.1:42449/status\" target=\"_blank\">http://127.0.0.1:42449/status</a>\n",
       "                        </td>\n",
       "                        <td style=\"text-align: left;\">\n",
       "                            <strong>Memory: </strong> 0 B\n",
       "                        </td>\n",
       "                    </tr>\n",
       "                    <tr>\n",
       "                        <td style=\"text-align: left;\">\n",
       "                            <strong>Nanny: </strong> tcp://127.0.0.1:34113\n",
       "                        </td>\n",
       "                        <td style=\"text-align: left;\"></td>\n",
       "                    </tr>\n",
       "                    <tr>\n",
       "                        <td colspan=\"2\" style=\"text-align: left;\">\n",
       "                            <strong>Local directory: </strong> /p/gpfs1/iopp/temp/local/dask-worker-space/worker-6ne6gk9n\n",
       "                        </td>\n",
       "                    </tr>\n",
       "\n",
       "                    \n",
       "\n",
       "                    \n",
       "\n",
       "                </table>\n",
       "            </details>\n",
       "            </div>\n",
       "        </div>\n",
       "        \n",
       "\n",
       "    </details>\n",
       "</div>\n",
       "\n",
       "        </details>\n",
       "    </div>\n",
       "</div>\n",
       "            </details>\n",
       "        \n",
       "\n",
       "    </div>\n",
       "</div>"
      ],
      "text/plain": [
       "<Client: 'tcp://127.0.0.1:46087' processes=8 threads=160>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "local_client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#file_client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wait_until_workers_alive(index, client, cluster, n_workers):\n",
    "        # Get current number of workers\n",
    "        current_n_workers = len(client.scheduler_info()['workers'])\n",
    "        # Wait until enough number of workers alive\n",
    "        while (client.status == 'running' and current_n_workers < n_workers):\n",
    "            # Print current status\n",
    "            print(f\"{current_n_workers}/{n_workers} workers running for client {index}\", end=\"\\r\")\n",
    "            # Try correcting state\n",
    "            cluster._correct_state()\n",
    "            # Sleep a little\n",
    "            sleep(5)\n",
    "            # Get current number of workers\n",
    "            current_n_workers = len(client.scheduler_info()['workers'])\n",
    "        #print(f\"All {n_workers} workers alive for client {index}\\r\\n\", end=\"\")\n",
    "async def keep_workers_alive(index, client, cluster, n_workers):\n",
    "        # While the job is still executing\n",
    "        while True:\n",
    "            # Wait a second\n",
    "            await asyncio.sleep(5)\n",
    "            # Check workers\n",
    "            wait_until_workers_alive(index, client, cluster, n_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0/8.0 workers running for client 0\r"
     ]
    }
   ],
   "source": [
    "for i, index in enumerate(filter_groups_indices):\n",
    "    wait_until_workers_alive(i, clients[index], clusters[index], n_workers_per_node/2)\n",
    "    keep_alive_task_2 = asyncio.create_task(keep_workers_alive(i, clients[index], clusters[index], n_workers_per_node/2))\n",
    "#wait_until_workers_alive(1, file_client, file_cluster, 4)\n",
    "#keep_alive_task_1 = asyncio.create_task(keep_workers_alive(1, file_client, file_cluster, n_workers))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spawn cluster nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#keep_alive_task_1.cancel()\n",
    "#keep_alive_task_2.cancel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis Utility Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Progress Bars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "pbar = ProgressBar()\n",
    "pbar.register()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Timer functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-16T21:44:25.856496Z",
     "start_time": "2021-09-16T21:44:25.849113Z"
    }
   },
   "outputs": [],
   "source": [
    "def timer(func):\n",
    "    @functools.wraps(func)\n",
    "    def wrapper_timer(*args, **kwargs):\n",
    "        tic = time.perf_counter()\n",
    "        value = func(*args, **kwargs)\n",
    "        toc = time.perf_counter()\n",
    "        elapsed_time = toc - tic\n",
    "        print(f\"Elapsed time: {elapsed_time:0.4f} seconds\")\n",
    "        return value\n",
    "    return wrapper_timer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Threading process "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-16T21:44:26.580383Z",
     "start_time": "2021-09-16T21:44:26.570029Z"
    }
   },
   "outputs": [],
   "source": [
    "def thread_print(string):\n",
    "    print(f'{string}\\n', end='')\n",
    "def thread_process( num_ranks, func, start=0, list_range=[], workers=40):\n",
    "    with ThreadPoolExecutor(max_workers = workers) as executor:\n",
    "        if len(list_range) == 0:\n",
    "            list_range = range(start, num_ranks) \n",
    "        future_gen = {executor.submit(func, rank): rank for rank in list_range}\n",
    "        for future in concurrent.futures.as_completed(future_gen):\n",
    "            rank = future_gen[future]\n",
    "            try:\n",
    "                data = future.result()\n",
    "            except Exception as exc:\n",
    "                thread_print('%r generated an exception: %s' % (rank, exc))\n",
    "            else:\n",
    "                thread_print('%r data computed' % (rank))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Application Logs in Parquet format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-16T21:44:27.037262Z",
     "start_time": "2021-09-16T21:44:27.033923Z"
    }
   },
   "outputs": [],
   "source": [
    "parquet_folder=\"/p/gpfs1/iopp/recorder_app_logs/genome_pegasus/nodes-32/_parquet\"\n",
    "#parquet_folder=\"/p/gpfs1/iopp/recorder_app_logs/hacc/nodes-32/_parquet\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "is_partitioned = os.path.exists(f\"{parquet_folder}/partitioned/_common_metadata\")\n",
    "print(is_partitioned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!rm -rf $parquet_folder/partitioned/*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!du -sh $parquet_folder/*\n",
    "#\n",
    "import glob\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5 µs, sys: 1 µs, total: 6 µs\n",
      "Wall time: 9.54 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "if  not is_partitioned:\n",
    "    files = glob.glob(f\"{parquet_folder}/partitioned/*.parquet\", recursive=True)\n",
    "    \n",
    "    for f in files:\n",
    "        os.remove(f)\n",
    "    files = glob.glob(f\"{parquet_folder}/*.parquet\", recursive=True)\n",
    "    file_size = 0\n",
    "    for f in files:\n",
    "        file_size = file_size + os.path.getsize(f)\n",
    "    ddf = dd.read_parquet(\"{}/*.parquet\".format(parquet_folder), engine=\"pyarrow-dataset\")\n",
    "    ddf = ddf.repartition(partition_size='4GB')\n",
    "    dd.to_parquet(ddf, f\"{parquet_folder}/partitioned/\")\n",
    "    #client.cancel(ddf)\n",
    "    files = glob.glob(f\"{parquet_folder}/partitioned/*.parquet\", recursive=True)\n",
    "    fs = 0\n",
    "    min_fs = -1\n",
    "    max_fs = 0\n",
    "    for f in files:\n",
    "        file_size = os.path.getsize(f)\n",
    "        if min_fs == -1 or min_fs > file_size:\n",
    "            min_fs = file_size\n",
    "        if max_fs < file_size:\n",
    "            max_fs = file_size\n",
    "        fs = fs + file_size\n",
    "\n",
    "    print(f\"{len(files)} files size: average :{fs/len(files)/1024.0/1024.0} min: {min_fs/1024.0/1024.0} max: {max_fs/1024.0/1024.0}\")\n",
    "    is_partitioned = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "103\n"
     ]
    }
   ],
   "source": [
    "!ls -l $parquet_folder/partitioned/* | wc -l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "101 file size: average :557.1953690500543 min: 66.02931213378906 max: 813.728705406189\n"
     ]
    }
   ],
   "source": [
    "files = glob.glob(f\"{parquet_folder}/partitioned/*.parquet\", recursive=True)\n",
    "fs = 0\n",
    "min_fs = -1\n",
    "max_fs = 0\n",
    "for f in files:\n",
    "    file_size = os.path.getsize(f)\n",
    "    if min_fs == -1 or min_fs > file_size:\n",
    "        min_fs = file_size\n",
    "    if max_fs < file_size:\n",
    "        max_fs = file_size\n",
    "    fs = fs + file_size\n",
    "\n",
    "print(f\"{len(files)} file size: average :{fs/len(files)/1024.0/1024.0} min: {min_fs/1024.0/1024.0} max: {max_fs/1024.0/1024.0}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import functools\n",
    "\n",
    "def minmax(aa):\n",
    "    return functools.reduce(lambda mm,xx : ( min(mm[0],xx),max(mm[1],xx)) , aa, ( aa[0],aa[0],))\n",
    "def load_json(filename, JSON_INDEX):\n",
    "    if filename:\n",
    "        f = open(filename)\n",
    "        data = json.load(f)\n",
    "        min_val, max_val = minmax(data[JSON_INDEX])\n",
    "        return [min_val, max_val]\n",
    "    else:\n",
    "        return [];\n",
    "\n",
    "def merge(x, y):\n",
    "    z = np.union1d(x, y)\n",
    "    del x\n",
    "    del y\n",
    "    return z\n",
    "\n",
    "def cal_len(x):\n",
    "    return len(x)\n",
    "\n",
    "def calculate_interval(INDEX, JSON_INDEX):\n",
    "    with clients[INDEX].as_current():\n",
    "        files = glob.glob(f\"{parquet_folder}/rank_*.json\", recursive=True)\n",
    "        TOTAL_JSON_FILES = len(files)\n",
    "        JSON_MAX_DEPTH = math.ceil(math.sqrt(TOTAL_JSON_FILES))\n",
    "        depth = list(range(0, JSON_MAX_DEPTH+1))\n",
    "        depth.reverse()\n",
    "        output = [0]*(JSON_MAX_DEPTH+1)\n",
    "        pieces = 2**JSON_MAX_DEPTH\n",
    "        non_delayed = []\n",
    "        for x in depth:\n",
    "            depth_ret = []\n",
    "            if (x == JSON_MAX_DEPTH):\n",
    "                for file in files:\n",
    "                    depth_ret.append(dask.delayed(load_json)(file, JSON_INDEX))\n",
    "            else:\n",
    "                elements_next_level = len(output[x+1])\n",
    "                num_elements = elements_next_level\n",
    "                if elements_next_level % 2 == 1:\n",
    "                    num_elements = num_elements - 1\n",
    "                for i in range(0, num_elements , 2):\n",
    "                    depth_ret.append([dask.delayed(min)(output[x+1][i][0], output[x+1][i+1][0]), dask.delayed(max)(output[x+1][i][1], output[x+1][i+1][1])])\n",
    "                if elements_next_level % 2 == 1:\n",
    "                    depth_ret.append(output[x+1][elements_next_level - 1])\n",
    "                output[x+1] = 0\n",
    "            output[x] = depth_ret\n",
    "        string_list = dask.compute(output)\n",
    "    min_val, max_val = string_list[0][0][0][0], string_list[0][0][0][1]\n",
    "    return min_val, max_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def filter_array(INDEX, processlist, column):\n",
    "#     if len(processlist) == 0:\n",
    "#         return []\n",
    "#     ddf = filter_groups_ddf[INDEX]\n",
    "#     good_keys = ddf.index.intersection(processlist)\n",
    "#     file_ddf = ddf.loc[good_keys]\n",
    "#     read_df, write_df, metadata_df = dask.compute(file_ddf[(file_ddf['io_cat'] == 1)][column].unique(),\n",
    "#                                                   file_ddf[(file_ddf['io_cat'] == 2)][column].unique(),\n",
    "#                                                   file_ddf[(file_ddf['io_cat'] == 3)][column].unique())\n",
    "#     del file_ddf \n",
    "#     return read_df\n",
    "def filter_interval(INDEX, start, end, column):\n",
    "    #order is important we want to first select correct time range then do filters.\n",
    "    ddf = filter_groups_ddf[INDEX]\n",
    "    time_df = ddf.loc[start:end]\n",
    "    read_df, write_df, metadata_df = dask.compute(time_df[(time_df['io_cat'] == 1)][column].unique(),\n",
    "                                                  time_df[(time_df['io_cat'] == 2)][column].unique(),\n",
    "                                                  time_df[(time_df['io_cat'] == 3)][column].unique())\n",
    "    del time_df \n",
    "    return read_df\n",
    "\n",
    "def calculate_metrics_interval(INDEX, MAX_DEPTH, COLUMN, MIN, MAX):\n",
    "    with clients[INDEX].as_current():\n",
    "        pieces = 2**MAX_DEPTH\n",
    "        interval = math.floor(MAX*1.0/pieces)\n",
    "        depth = list(range(0, MAX_DEPTH+1))\n",
    "        depth.reverse()\n",
    "        output = [0]*(MAX_DEPTH+1)\n",
    "        time_range = np.arange(MIN, MAX, interval)\n",
    "        for x in depth:\n",
    "            depth_ret = []\n",
    "            if (x == MAX_DEPTH):\n",
    "                for i in time_range:\n",
    "                    #print(i)\n",
    "                    depth_ret.append(dask.delayed(filter_interval)(INDEX, i, i + interval, COLUMN))\n",
    "            else:\n",
    "                pieces = len(output[x+1])\n",
    "                if pieces % 2 == 1:\n",
    "                    pieces = pieces - 1\n",
    "                for i in range(0, pieces , 2):\n",
    "                    depth_ret.append(dask.delayed(merge)(output[x+1][i], output[x+1][i+1]))\n",
    "                pieces = len(output[x+1])\n",
    "                if pieces % 2 == 1:\n",
    "                    depth_ret.append(output[x+1][pieces - 1])\n",
    "                for index, val in enumerate(output[x+1]):\n",
    "                    output[x+1][index] = dask.delayed(cal_len)(val)\n",
    "            output[x] = depth_ret\n",
    "        metrics = dask.compute(output)\n",
    "    return metrics\n",
    "# def calculate_metrics_array(INDEX, MAX_DEPTH, JSON_INDEX, column):\n",
    "    \n",
    "#     with clients[INDEX].as_current():\n",
    "#         str_list = calculate_strings(INDEX, JSON_INDEX)\n",
    "#         pieces = 2**MAX_DEPTH\n",
    "#         interval = math.floor(len(str_list)*1.0/pieces)\n",
    "#         if interval == 0:\n",
    "#             pieces = len(str_list)\n",
    "#             interval = 1\n",
    "#             MAX_DEPTH = math.ceil(math.sqrt(pieces))\n",
    "#         depth = list(range(0, MAX_DEPTH+1))\n",
    "#         depth.reverse()\n",
    "#         output = [0]*(MAX_DEPTH+1)\n",
    "#         non_delayed = []\n",
    "#         for x in depth:\n",
    "#             depth_ret = []\n",
    "#             if (x == MAX_DEPTH):\n",
    "#                 for i in range(0, len(str_list), interval):\n",
    "#                     #print(i,  i + interval)\n",
    "#                     depth_ret.append(dask.delayed(filter_array)(INDEX, str_list[i: i + interval], column))\n",
    "#             else:\n",
    "#                 elements_next_level = len(output[x+1])\n",
    "#                 elements = elements_next_level\n",
    "#                 if elements_next_level % 2 == 1:\n",
    "#                     elements = elements - 1\n",
    "#                 for i in range(0, elements , 2):\n",
    "#                     depth_ret.append(dask.delayed(merge)(output[x+1][i], output[x+1][i+1]))\n",
    "#                 if elements_next_level % 2 == 1:\n",
    "#                     depth_ret.append(output[x+1][elements_next_level - 1])\n",
    "#                 for index, val in enumerate(output[x+1]):\n",
    "#                     output[x+1][index] = dask.delayed(cal_len)(val)\n",
    "#             output[x] = depth_ret\n",
    "#         metrics = dask.compute(output)\n",
    "#     return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import dask\n",
    "import math\n",
    "import dask.array as da\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_process_metrics(MAX_DEPTH):\n",
    "    INDEX = 'proc_id'\n",
    "    JSON_INDEX = 'processes'\n",
    "    COLUMN = 'file_id'\n",
    "    MIN, MAX = calculate_interval(INDEX, JSON_INDEX)\n",
    "    process_metrics = calculate_metrics_interval(INDEX, MAX_DEPTH, COLUMN, MIN, MAX)\n",
    "    return process_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def compute_file_metrics(MAX_DEPTH):\n",
    "    INDEX = 'file_id'\n",
    "    JSON_INDEX = 'filenames'\n",
    "    COLUMN = 'proc_id'\n",
    "    MIN, MAX = calculate_interval(INDEX, JSON_INDEX)\n",
    "    file_metrics = calculate_metrics_interval(INDEX, MAX_DEPTH, COLUMN, MIN, MAX)\n",
    "    return file_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_timeline_metrics(MAX_DEPTH):\n",
    "    INDEX = 'tmid'\n",
    "    COLUMN = 'proc_id'\n",
    "    f = open(f'{parquet_folder}/global.json')\n",
    "    data = json.load(f)\n",
    "    MIN, MAX = 0, math.ceil(data['max_tend'])\n",
    "    print(MIN, MAX)\n",
    "    timeline_metrics = calculate_metrics_interval(INDEX, MAX_DEPTH, COLUMN, MIN, MAX)\n",
    "    return timeline_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_DEPTH = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cluster.scale(n_workers_per_node*4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# dataframe_memory = ddf.memory_usage(deep=True).compute()\n",
    "# import psutil\n",
    "# vmem = psutil.virtual_memory()\n",
    "# print(vmem)\n",
    "# dataframe_memory_gb = dataframe_memory.sum()\n",
    "# is_computed = False\n",
    "# if dataframe_memory_gb < vmem[0] *.70:\n",
    "#     is_computed = True\n",
    "# print(f\"We should compute: {is_computed}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# %%time\n",
    "# string_list = {}\n",
    "# for i, index in enumerate(filter_groups_indices):\n",
    "#     with clients[index].as_current():\n",
    "#         string_list[index] = []\n",
    "# with concurrent.futures.ThreadPoolExecutor(max_workers = 2) as executor:\n",
    "#     index_futures = {executor.submit(calculate_strings, index, json_index): index for json_index, index in [['filenames','filename'], ['processes','hostname']]}\n",
    "#     for future in concurrent.futures.as_completed(index_futures):\n",
    "#         index = index_futures[future]\n",
    "#         try:\n",
    "#             result = future.result()\n",
    "#         except Exception as exc:\n",
    "#             print(f'{index} generated an exception: {exc}')\n",
    "#         else:\n",
    "#             print(f'Created strs from json for {index} with len {result}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%time\n",
    "#clients['filename'].scatter(all_files)\n",
    "#clients['hostname'].scatter(processes_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reindex(INDEX):\n",
    "    with clients[INDEX].as_current():\n",
    "        ddf =  dd.read_parquet(f\"{parquet_folder}/partitioned/*.parquet\", engine=\"pyarrow-dataset\", index=False)\n",
    "        ddf_indexed = ddf.set_index([INDEX])\n",
    "    ddf_indexed = clients[INDEX].persist(ddf_indexed)\n",
    "    #result = wait(ddf_indexed)\n",
    "    filter_groups_ddf[INDEX] = ddf_indexed\n",
    "    return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created index for tmid\n",
      "Created index for file_id\n",
      "Created index for proc_id\n",
      "CPU times: user 23.5 s, sys: 3.88 s, total: 27.4 s\n",
      "Wall time: 45.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "filter_groups_ddf = {}\n",
    "with concurrent.futures.ThreadPoolExecutor(max_workers = len(filter_groups_indices)) as executor:\n",
    "    index_futures = {executor.submit(reindex, index): index for index in filter_groups_indices}\n",
    "    for future in concurrent.futures.as_completed(index_futures):\n",
    "        index = index_futures[future]\n",
    "        try:\n",
    "            result = future.result()\n",
    "        except Exception as exc:\n",
    "            print(f'{index} generated an exception: {exc}')\n",
    "        else:\n",
    "            print(f'Created index for {index}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#%%time\n",
    "#compute_timeline_metrics(MAX_DEPTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%time\n",
    "#compute_file_metrics(MAX_DEPTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%time\n",
    "#compute_process_metrics(MAX_DEPTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 318289967883\n",
      "[-8962162906279966645 -8962160363659327413 -8962159706529331125\n",
      " -8962157696484636597 -8962156712937125813]\n",
      "Calculated metrics for filtergroup with index: tmid\n",
      "[-8962162906279966645 -8962160363659327413 -8962159706529331125\n",
      " -8962157696484636597 -8962156712937125813]\n",
      "Calculated metrics for filtergroup with index: file_id\n",
      "[-7497085197234279354 -7497085197232924232 -7497085197232544690\n",
      " -7497085197232062185 -7497085197231812390]\n",
      "Calculated metrics for filtergroup with index: proc_id\n",
      "CPU times: user 4min 54s, sys: 1min 2s, total: 5min 57s\n",
      "Wall time: 5min 16s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "filter_group_funcs = {'tmid':compute_timeline_metrics, 'proc_id':compute_process_metrics, 'file_id':compute_file_metrics}\n",
    "filter_groups_metrics = {}\n",
    "with concurrent.futures.ThreadPoolExecutor(max_workers = len(filter_groups_indices)) as executor:\n",
    "    index_futures = {executor.submit(filter_group_funcs[index], MAX_DEPTH): index for index in filter_groups_indices}\n",
    "    for future in concurrent.futures.as_completed(index_futures):\n",
    "        index = index_futures[future]\n",
    "        try:\n",
    "            filter_groups_metrics[index] = future.result()\n",
    "            print(filter_groups_metrics[index][0][0][0][:5])\n",
    "        except Exception as exc:\n",
    "            print(f'{index} generated an exception: {exc}')\n",
    "        else:\n",
    "            print(f'Calculated metrics for filtergroup with index: {index}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with clients['tmid'].as_current():\n",
    "    print(filter_groups_ddf['tmid'].index.unique().compute())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "process_future = dask.delayed(compute_process_metrics)(MAX_DEPTH)\n",
    "process_metrics, processes_all = process_future.compute()\n",
    "print(process_metrics[0][0][0][:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "file_future = dask.delayed(compute_file_metrics)(MAX_DEPTH)\n",
    "file_metrics, files_all = file_future.compute()\n",
    "print(file_metrics[0][0][0][:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "time_future = dask.delayed(compute_timeline_metrics)(MAX_DEPTH)\n",
    "timeline_metrics, time_range = time_future.compute()\n",
    "print(timeline_metrics[0][0][0][:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster.scale(n_workers_per_node*2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "process_future = local_client.submit(compute_process_metrics, MAX_DEPTH)\n",
    "process_metrics, processes_all = process_future.result()\n",
    "print(process_metrics[0][0][0][:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "file_future = local_client.submit(compute_file_metrics, MAX_DEPTH)\n",
    "file_metrics, files_all = file_future.result()\n",
    "print(file_metrics[0][0][0][:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "time_future = local_client.submit(compute_timeline_metrics, MAX_DEPTH)\n",
    "timeline_metrics, time_range = time_future.result()\n",
    "print(timeline_metrics[0][0][0][:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "if_future = False\n",
    "#load_timeline_future = timeline_client.submit(compute_timeline_metrics, 10)\n",
    "#load_file_future = file_client.submit(compute_file_metrics, 10)\n",
    "if if_future:\n",
    "    file_client.cancel(load_process_future)\n",
    "load_process_future = file_client.submit(compute_process_metrics, 3)\n",
    "if_future = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "#timeline_values = load_timeline_future.result()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#file_cluster.scale(16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "processes_all, process_metrics = load_process_future.result()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(len(process_metrics[0][0][0]))\n",
    "for x in range(1,4):\n",
    "    print(\"depth \",x,\" has \", len(process_metrics[0][x-1]), \" timesteps\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_DEPTH = 5\n",
    "import dask\n",
    "def filter(filelist):\n",
    "    #order is important we want to first select correct time range then do filters.\n",
    "    file_ddf = ddf.loc[filelist]\n",
    "    read_df, write_df, metadata_df = dask.compute(file_ddf[(file_ddf['io_cat'] == 1)]['hostname'].unique(),\n",
    "                                                  file_ddf[(file_ddf['io_cat'] == 2)]['hostname'].unique(),\n",
    "                                                  file_ddf[(file_ddf['io_cat'] == 3)]['hostname'].unique())\n",
    "    del file_ddf \n",
    "    return read_df\n",
    "\n",
    "def merge(x, y):\n",
    "    return np.union1d(x, y)\n",
    "\n",
    "depth = list(range(0, MAX_DEPTH+1))\n",
    "depth.reverse()\n",
    "pieces = 2**MAX_DEPTH\n",
    "interval = math.floor(len(global_files[0][0])*1.0/pieces)\n",
    "print(tend, interval)\n",
    "print(depth, pieces)\n",
    "output = [0]*(MAX_DEPTH+1)\n",
    "print(output)\n",
    "for x in depth:\n",
    "    depth_ret = []\n",
    "    if (x == MAX_DEPTH):\n",
    "        for i in range(0, len(), interval):\n",
    "            #print(i,  i + interval)\n",
    "            depth_ret.append(dask.delayed(filter)(global_files[0][0][i: i + interval]))\n",
    "    else:\n",
    "        pieces_in_next_level = 2**(x+1)\n",
    "        for i in range(0, pieces_in_next_level , 2):\n",
    "            depth_ret.append(dask.delayed(merge)(output[x+1][i], output[x+1][i+1]))\n",
    "    output[x] = depth_ret\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "result = wait(ddf_filename, timeline_ddf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#ddf.groupby(['app','func_id'])['func_id'].count().compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#ddf = dd.read_parquet(\"{}/*.parquet\".format(parquet_folder), engine=\"pyarrow-dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#time_df = ddf[ddf['func_id'].isin([\"read\"])].sample(frac=0.01).compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#time_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#time_df[(time_df['io_cat'] == 2)].compute().head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#time_df[(time_df['io_cat'] == 3)].compute().head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#timeline_cluster.scale(n_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "with timeline_client.as_current():\n",
    "    total = dask.delayed(output)()\n",
    "total.visualize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "with timeline_client.as_current():\n",
    "    \n",
    "    result = wait(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "#result = wait(ddf_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#file_ddf = ddf_filename[ddf_filename['size'] > 0].sample(frac=0.01).compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#file_ddf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#file_ddf.loc[[\"/p/gpfs1/iopp/temp/1000-genome-haridev/scratch/run_dir/ALL.chr1.250000.vcf\",\"/p/gpfs1/iopp/temp/1000-genome-haridev/scratch/run_dir/ALL.chr2.250000.vcf\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#f = open(f'{parquet_folder}/rank_0.json')\n",
    "#data = json.load(f)\n",
    "#filenames = data[\"filenames\"]\n",
    "#print(len(filenames))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = glob.glob(f\"{parquet_folder}/rank_*.json\", recursive=True)\n",
    "TOTAL_JSON_FILES = len(files)\n",
    "MAX_DEPTH = math.ceil(math.sqrt(TOTAL_JSON_FILES))\n",
    "print(MAX_DEPTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "         \n",
    "with file_client.as_current():\n",
    "    total = dask.delayed(output)()\n",
    "total.visualize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "         \n",
    "with file_client.as_current():\n",
    "    global_files = dask.compute(output)\n",
    "#result = wait(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(global_files[0][0])\n",
    "for x in depth:\n",
    "    print(\"depth \",x,\" has \", len(global_files[0][x-1]), \" timesteps\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "         \n",
    "with file_client.as_current():\n",
    "    total = dask.delayed(output)()\n",
    "total.visualize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "         \n",
    "with file_client.as_current():\n",
    "    global_files = dask.compute(output)\n",
    "#result = wait(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(global_files[0][0])\n",
    "for x in depth:\n",
    "    print(\"depth \",x,\" has \", len(global_files[0][x-1]), \" timesteps\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tstart=0\n",
    "tend=33\n",
    "MAX_DEPTH=1\n",
    "def recurse(depth=-1):\n",
    "    depth+=1;\n",
    "    return_vals = []\n",
    "    vals = {}\n",
    "    if (depth < MAX_DEPTH):\n",
    "        vals =  dask.delayed(recurse)(depth)\n",
    "    if depth == MAX_DEPTH:\n",
    "        pieces = 2**depth\n",
    "        print(pieces)\n",
    "        interval = math.floor((tend-tstart)*1.0/pieces)\n",
    "        for i in range(tstart, tend, interval):\n",
    "            #range_ddf = ddf[(ddf['tstart']>= i) & (ddf['tend']< (i+interval))]\n",
    "            #range_ddf = range_ddf.to_delayed(optimize_graph=True)\n",
    "            val = dask.delayed(np.ones)((4, 4))\n",
    "            return_vals.append(val)\n",
    "    elif (depth < MAX_DEPTH):\n",
    "        pieces_in_next_level = 2**(depth+1)\n",
    "        for i in range(0, pieces_in_next_level-1, 2):\n",
    "            data = [da.from_array(vals[j]) for j in range(i,i+1)]\n",
    "            return_vals.append(da.concatenate(data, axis=0))\n",
    "    vals = return_vals\n",
    "    return vals\n",
    "import dask\n",
    "total = dask.delayed(recurse)()\n",
    "total.visualize()\n",
    "#total.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tstart=0\n",
    "tend=33\n",
    "MAX_DEPTH=0\n",
    "def recurse(ddf, depth=-1):\n",
    "    depth+=1;\n",
    "    return_vals = []\n",
    "    vals = {}\n",
    "    #if (depth < MAX_DEPTH):\n",
    "    #    vals = recurse(ddf, depth)\n",
    "    if depth == MAX_DEPTH:\n",
    "        pieces = 2**depth\n",
    "        print(pieces)\n",
    "        interval = math.floor((tend-tstart)*1.0/pieces)\n",
    "        for i in range(tstart, tend, interval):\n",
    "            #range_ddf = ddf[(ddf['tstart']>= i) & (ddf['tend']< (i+interval))]\n",
    "            #range_ddf = range_ddf.to_delayed(optimize_graph=True)\n",
    "            val =[1,2] #range_ddf['rank'].unique()\n",
    "            return_vals.append(val)\n",
    "    elif (depth < MAX_DEPTH):\n",
    "        pieces_in_next_level = 2**(depth+1)\n",
    "        for i in range(0, pieces_in_next_level-1, 2):\n",
    "            return_vals.append(np.union1d(vals[str(depth+1)][i],vals[str(depth+1)][i+1]))\n",
    "    vals[str(depth)] = return_vals\n",
    "    return vals\n",
    "import dask\n",
    "total = dask.delayed(recurse)(ddf)\n",
    "total.visualize()\n",
    "#vals = total.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-16T21:44:28.361097Z",
     "start_time": "2021-09-16T21:44:28.356562Z"
    }
   },
   "outputs": [],
   "source": [
    "num_ranks = 160"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial Boostrapping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read Application logs into dask dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Dask dataframe is split into {} partitions\".format(ddf.npartitions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-16T21:37:12.311012Z",
     "start_time": "2021-09-16T21:37:12.306571Z"
    }
   },
   "outputs": [],
   "source": [
    "print(\"Columns present in dataset\")\n",
    "print(ddf.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-16T21:37:12.873991Z",
     "start_time": "2021-09-16T21:37:12.871121Z"
    }
   },
   "outputs": [],
   "source": [
    "ddf.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Only keep level zero from MPI and FTRACE but keep all levels for I/O"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-16T21:37:13.310962Z",
     "start_time": "2021-09-16T21:37:13.306458Z"
    }
   },
   "outputs": [],
   "source": [
    "ddf = ddf[(ddf['level'] == 0) | ddf['cat'].isin([0,1,3])]\n",
    "g1 = ddf.__dask_graph__()\n",
    "g1.layers\n",
    "#ddf = ddf.persist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split dataframe into I/O, MPI, and trace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "io_ddf = ddf[ddf['cat'].isin([0,1,3])]\n",
    "exclude_dirs = [\".pyc\", \"/usr\",\"/g/g92/haridev/.nccl.conf\"]\n",
    "for directory in exclude_dirs:\n",
    "    io_ddf = io_ddf[~io_ddf['args_1'].str.contains(directory)]\n",
    "#io_ddf[rank] = io_ddf[rank].persist()\n",
    "mpi_ddf = ddf[ddf['cat'] == 2]\n",
    "#mpi_ddf[rank] = mpi_ddf[rank].persist()\n",
    "trace_ddf = ddf[ddf['cat'] == 4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check I/O interfaces used by application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interfaces = io_ddf['func_id'].unique().compute()\n",
    "print(interfaces)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "Finding"
    ]
   },
   "source": [
    "Only POSIX interface is used in this application"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set duration of I/O events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "io_ddf['duration'] = io_ddf['tend'] - io_ddf['tstart']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "negetive_duration = io_ddf[io_ddf['duration'] < 0]['duration'].count().compute()\n",
    "print(\"Application Profiler has stored {} negetive values\".format(negetive_duration))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create conditions for filtering\n",
    "Even though this application doesnt have MPI or STDIO we have all the conditions here. This is to make sure we cover other apps as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "open_condition = io_ddf['func_id'].str.contains(\"open\")\n",
    "mpi_condition = io_ddf['func_id'].str.contains(\"MPI\")\n",
    "read_condition = io_ddf['func_id'].isin([\"read\", \"pread\", \"pread64\", \"readv\"])\n",
    "fread_condition = io_ddf['func_id'].isin([\"fread\"])\n",
    "close_condition = io_ddf['func_id'].str.contains('close')\n",
    "write_condition = io_ddf['func_id'].isin([\"write\", \"pwrite\", \"pwrite64\",\"writev\"])\n",
    "fwrite_condition = io_ddf['func_id'].isin([\"fwrite\"])\n",
    "readdir_condition = io_ddf['func_id'].isin([\"readdir\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract Filename from each row.\n",
    "This simplifies analysis later on as we can filter recorder per rank and per file to understand access pattern information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "io_ddf['filename'] = \"\"\n",
    "io_ddf['filename'] = io_ddf['filename'].mask(open_condition & ~mpi_condition, io_ddf['args_1'])\n",
    "io_ddf['filename'] = io_ddf['filename'].mask(open_condition & mpi_condition, io_ddf['args_2'])\n",
    "io_ddf['filename'] = io_ddf['filename'].mask(close_condition, io_ddf['args_1'])\n",
    "io_ddf['filename'] = io_ddf['filename'].mask(read_condition, io_ddf['args_1'])\n",
    "io_ddf['filename'] = io_ddf['filename'].mask(fread_condition, io_ddf['args_4'])\n",
    "io_ddf['filename'] = io_ddf['filename'].mask(write_condition, io_ddf['args_1'])\n",
    "io_ddf['filename'] = io_ddf['filename'].mask(fwrite_condition, io_ddf['args_4'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some filename has redundent forward slashes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "io_ddf['filename'] = io_ddf['filename'].str.replace('//','/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter recorder which have read and write operations and which have metadata.\n",
    "We assume that there is only two types of operation in I/O read/write and metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "io_ddf_read_write = io_ddf[read_condition | write_condition].compute()\n",
    "io_ddf_metadata = io_ddf[~read_condition & ~write_condition].compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build size and count to understand I/O on the io_ddf_read_write dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "open_condition = io_ddf_read_write['func_id'].str.contains(\"open\")\n",
    "mpi_condition = io_ddf_read_write['func_id'].str.contains(\"MPI\")\n",
    "read_condition = io_ddf_read_write['func_id'].isin([\"read\", \"pread\", \"pread64\", \"readv\"])\n",
    "fread_condition = io_ddf_read_write['func_id'].isin([\"fread\"])\n",
    "close_condition = io_ddf_read_write['func_id'].str.contains('close')\n",
    "write_condition = io_ddf_read_write['func_id'].isin([\"write\", \"pwrite\", \"pwrite64\",\"writev\"])\n",
    "fwrite_condition = io_ddf_read_write['func_id'].isin([\"fwrite\"])\n",
    "readdir_condition = io_ddf_read_write['func_id'].isin([\"readdir\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "io_ddf_read_write['size'] = 0\n",
    "io_ddf_read_write['count'] = 1\n",
    "io_ddf_read_write['size'] = io_ddf_read_write['size'].mask(read_condition , io_ddf_read_write['args_3'])\n",
    "io_ddf_read_write['size'] = io_ddf_read_write['size'].mask(fread_condition, io_ddf_read_write['args_3'])\n",
    "io_ddf_read_write['count'] = io_ddf_read_write['count'].mask(fread_condition, io_ddf_read_write['args_2'])\n",
    "io_ddf_read_write['size'] = io_ddf_read_write['size'].mask(write_condition, io_ddf_read_write['args_3'])\n",
    "io_ddf_read_write['size'] = io_ddf_read_write['size'].mask(fwrite_condition, io_ddf_read_write['args_3'])\n",
    "io_ddf_read_write['count'] = io_ddf_read_write['count'].mask(fwrite_condition, io_ddf_read_write['args_2'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Corner cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "io_ddf_read_write['size'] = io_ddf_read_write['size'].mask(readdir_condition , \"0\")\n",
    "io_ddf_read_write['count'] = io_ddf_read_write['count'].mask(readdir_condition , \"1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set Datatypes\n",
    "This is needed as by default args are string from which they are set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "io_ddf_read_write = io_ddf_read_write.astype({'size': 'int32','count': 'int32'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compute Final Size\n",
    "After this point we do not use count field."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "io_ddf_read_write['size'] = io_ddf_read_write['size'] * io_ddf_read_write['count']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate Bandwidth in MB/s for I/O operations.\n",
    "Durations might be incorrect or equal to zero due to profilers resolution issues. Therefore, we do bandwidth calculations for only correct durations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_dur = ((io_ddf_read_write['tend'] - io_ddf_read_write['tstart']) > 0)\n",
    "io_ddf_read_write['bandwidth'] = 0\n",
    "io_ddf_read_write['bandwidth'] = io_ddf_read_write['bandwidth'].mask(correct_dur, io_ddf_read_write['size']*1.0/(io_ddf_read_write['tend'] - io_ddf_read_write['tstart'])/1024.0/1024.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "Finding"
    ]
   },
   "source": [
    "### Calculate all relevent filenames in the application\n",
    "The profiler might see jsrun files and other irrelevant internal files which we filter here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_filenames = io_ddf[io_ddf['filename'].str.contains('/p/gpfs1')]['filename'].unique().compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "Finding"
    ]
   },
   "outputs": [],
   "source": [
    "print(\"The application accesses {} files across all ranks\".format(len(all_filenames)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "Finding"
    ]
   },
   "source": [
    "### Distribution of Transfer Size and Bandwidth in the application\n",
    "The transfer size is given by the size column in bytes and the bandwidth is shown in MB/s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "Finding"
    ]
   },
   "outputs": [],
   "source": [
    "io_ddf_read_write[['size', 'bandwidth']].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preload the I/O dataframe for faster analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "io_ddf = io_ddf.persist()\n",
    "result = wait(io_ddf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "Finding"
    ]
   },
   "source": [
    "## Application Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Num Ranks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "Finding"
    ]
   },
   "outputs": [],
   "source": [
    "num_ranks = io_ddf_read_write['rank'].nunique()\n",
    "print(\"Number of ranks in application {}\".format(num_ranks))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Total I/O amount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "Finding"
    ]
   },
   "outputs": [],
   "source": [
    "total_io = io_ddf_read_write.groupby('func_id')['size'].sum()/1024.0/1024.0/1024.0\n",
    "print(\"Total I/O in application {} GB\".format(total_io.sum()))\n",
    "print(\"Total I/O per operation in application {} GB\".format(total_io))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I/O per rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "io_ddf_read_write.groupby(['rank', 'func_id'])['size'].sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "Finding"
    ]
   },
   "source": [
    "Every rank performs the same amount of I/O and is evenly distributed amoung the read and write operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "io_per_rank_gb = io_ddf_read_write.groupby(['rank'])['size'].sum()/1024.0/1024.0/1024.0\n",
    "io_per_rank_gb.plot(kind='line',figsize=(4,4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Operations Per Rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "io_per_rank_gb = io_ddf_read_write.groupby('rank')['count'].sum()\n",
    "io_per_rank_gb.plot(kind='line',figsize=(4,4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Average I/O Time per process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_per_rank = io_ddf_read_write.groupby('rank')['duration'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"time per process: Average {} sec, Max {} sec, and Min {} sec\".format(time_per_rank.mean(), time_per_rank.max(), time_per_rank.min()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_per_rank.plot(kind='line',figsize=(4,4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "Finding"
    ]
   },
   "source": [
    "Even though every rank has identical I/O (operations and amount), they have a variable performance.\n",
    "- **Possible Reason:**\n",
    "  - Overwelming of PFS from I/O parallelism.\n",
    "  - Transfer size doesnt match Stripe size."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute Time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convert trace dataframe into memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trace_ddf_local = trace_ddf.compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate duration for trace values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trace_ddf_local['duration'] = trace_ddf_local['tend']-trace_ddf_local['tstart']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "Finding"
    ]
   },
   "outputs": [],
   "source": [
    "negetive_durations = trace_ddf_local[trace_ddf_local['duration'] < 0]['duration'].count()\n",
    "print(\"Profiler stored {} negetive rows\".format(negetive_durations))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_per_rank = trace_ddf_local.groupby('rank')['duration'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "Finding"
    ]
   },
   "outputs": [],
   "source": [
    "print(\"Time per process: Average {} sec, Max {} sec, and Min {} sec\".format(time_per_rank.mean(), time_per_rank.max(), time_per_rank.min()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_per_rank.plot(kind='line',figsize=(4,4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Job Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_time = ddf['tend'].max().compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Job Time in Application {} sec\".format(job_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The application run for a long time includes initialization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Job Intensity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_compute = trace_ddf_local.groupby('rank')['duration'].sum().max()\n",
    "total_io = io_ddf.groupby('rank')['duration'].sum().max().compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "io_intensity=total_io/(total_io + total_compute)\n",
    "comp_intensity=total_compute/(total_io + total_compute)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"I/O intensity: {}, Compute intensity: {}\".format(io_intensity, comp_intensity))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time spent in Data Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_ops = ['read', 'write']\n",
    "total_time_io_data = io_ddf[io_ddf['func_id'].isin(data_ops)].groupby('rank')['duration'].sum().max().compute()\n",
    "total_time_io_metadata = io_ddf[~io_ddf['func_id'].isin(data_ops)].groupby('rank')['duration'].sum().max().compute()\n",
    "print(\"Time spent on data: {} sec, metadata: {} sec\".format(total_time_io_data, total_time_io_metadata))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I/O Transfer Size Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_df_temp = io_ddf_read_write\n",
    "size_ranges = [0, 4*1024, 64*1024, 1024*1024, 16*1024*1024]\n",
    "size_ranges_str = []\n",
    "for range_val in size_ranges:\n",
    "    size_ranges_str.append(str(range_val))\n",
    "max_range = len(size_ranges)\n",
    "request_size = [0]*len(size_ranges)\n",
    "for i, val in enumerate(size_ranges):\n",
    "    #print(i, max_range)\n",
    "    if i < max_range - 1:\n",
    "        request_size[i] = file_df_temp[(file_df_temp['size'].ge(val)) & \n",
    "                                       (file_df_temp['size'].lt(size_ranges[i+1]))]['size'].count()\n",
    "    else:\n",
    "        request_size[i] = file_df_temp[(file_df_temp['size'] >=size_ranges[i])]['size'].count()\n",
    "\n",
    "print(request_size)\n",
    "plt.bar(size_ranges_str, request_size)\n",
    "#file_sizes = file_df_temp['size'].to_numpy() / 1024.0 /1024.0\n",
    "#plt.hist(file_sizes, bins=100)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "Finding"
    ]
   },
   "source": [
    "The application uses a constant transfer size of 16MB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distribution of Bandwidth achived by application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bw_ranges = [1, 128, 1024, 1024*64]\n",
    "bw_ranges_str = []\n",
    "for range_val in bw_ranges:\n",
    "    bw_ranges_str.append(str(range_val))\n",
    "max_range = len(bw_ranges)\n",
    "request_bw = [0]*len(bw_ranges)\n",
    "for i, val in enumerate(bw_ranges):\n",
    "    #print(i, max_range)\n",
    "    if i < max_range - 1:\n",
    "        request_bw[i] = file_df_temp[(file_df_temp['bandwidth'].ge(val)) & \n",
    "                                       (file_df_temp['bandwidth'].lt(bw_ranges[i+1]))]['size'].count()\n",
    "    else:\n",
    "        request_bw[i] = file_df_temp[(file_df_temp['bandwidth'] >=bw_ranges[i])]['size'].count()\n",
    "fig, ax = plt.subplots()\n",
    "ax.barh(bw_ranges_str, request_bw)\n",
    "for i, v in enumerate(request_bw):\n",
    "    ax.text(v + 3, i + .25, str(v), color='blue', fontweight='bold')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "request_bw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "percentage_bw = np.array(request_bw)*100/np.sum(request_bw)\n",
    "percentage_bw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ranks = range(40)\n",
    "ranks_str = []\n",
    "for rank in ranks:\n",
    "    ranks_str.append(str(rank))\n",
    "size_ranges = [0, 4*1024, 64*1024, 1024*1024, 16*1024*1024]\n",
    "request_size = [[]]*len(size_ranges)\n",
    "for i, val in enumerate(size_ranges):\n",
    "    request_size[i] = [0]*len(ranks)\n",
    "    for j,rank in enumerate(ranks):\n",
    "        \n",
    "        file_df_temp = io_ddf_read_write[io_ddf_read_write['rank'] == rank]\n",
    "        max_range = len(size_ranges)\n",
    "        if i < max_range - 1:\n",
    "            request_size[i][j] = file_df_temp[(file_df_temp['size'].ge(val)) & \n",
    "                                           (file_df_temp['size'].lt(size_ranges[i+1]))]['bandwidth'].mean()\n",
    "        else:\n",
    "            request_size[i][j] = file_df_temp[(file_df_temp['size'] >=size_ranges[i])]['bandwidth'].mean()\n",
    "#fig, ax = plt.subplots(figsize=(16,4))\n",
    "#width = 0.35\n",
    "#plt.figure()\n",
    "for i, val in enumerate(size_ranges):\n",
    "    plt.figure(figsize=(16,4))\n",
    "    print(request_size[i][0],np.mean(request_size[i]))\n",
    "    rects1 = plt.bar(ranks_str, request_size[i], 0.35, label=str(i))\n",
    "    plt.show()\n",
    "ax.legend()\n",
    "\n",
    "fig.tight_layout()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "Finding"
    ]
   },
   "source": [
    "- 71% of the overall I/O got a bandwidth of 1 GB/s per process.\n",
    "- 27% achieve a low bandwidth of 128MB/s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number of files read/written"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "Finding"
    ]
   },
   "outputs": [],
   "source": [
    "all_filenames = io_ddf[io_ddf['filename'].str.contains('/p/gpfs1')]['filename'].unique().compute()\n",
    "print(\"The application accesses {} files\".format(len(all_filenames)))\n",
    "#print(all_filenames[:8],all_filenames[-8:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I/O Operations by Application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "io_ddf.groupby('func_id')['func_id'].count().compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As there are 1280 ranks in total, the distribution of operations is even among the processes.\n",
    "We will confirm it with random inspections next."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IO Operations per rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "io_ddf.groupby(['rank','func_id'])['func_id'].count().compute()[:8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "io_ddf.groupby(['rank','func_id'])['func_id'].count().compute()[32:40]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "Finding"
    ]
   },
   "source": [
    "These confirm our hypothesis that HACC-IO performs same operations per process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bandwidth per request distribution\n",
    "We calculate the achived bandwidth per request size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ranks = range(num_ranks)\n",
    "size_ranges = [0, 4*1024, 64*1024, 1024*1024, 16*1024*1024]\n",
    "request_size = [[]]*len(size_ranges)\n",
    "for i, val in enumerate(size_ranges):\n",
    "    request_size[i] = [0]*num_ranks\n",
    "    for rank in range(num_ranks):\n",
    "        file_df_temp = io_ddf_read_write[io_ddf_read_write['rank'] == rank]\n",
    "        max_range = len(size_ranges)\n",
    "        if i < max_range - 1:\n",
    "            request_size[i][rank] = file_df_temp[(file_df_temp['size'].ge(val)) & \n",
    "                                           (file_df_temp['size'].lt(size_ranges[i+1]))]['bandwidth'].mean()\n",
    "        else:\n",
    "            request_size[i][rank] = file_df_temp[(file_df_temp['size'] >=size_ranges[i])]['bandwidth'].mean()\n",
    "fig, ax = plt.subplots()\n",
    "width = 0.35\n",
    "for i, val in enumerate(size_ranges):\n",
    "    rects1 = ax.bar(ranks, request_size[i], width, label=str(i))\n",
    "plt.show()\n",
    "ax.legend()\n",
    "\n",
    "fig.tight_layout()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the application has only one request size 16MB, we see a single line. However, we see variation of upto 10x between bandwidth from few ranks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overlapping of Compute and I/O"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "io_ddf.groupby('rank')['thread_id'].nunique().describe().compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### No additional threads means I/O is synchronous to compute. I.e. all I/O is unoverlapped."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Timeline\n",
    "The timeline analysis shows how each rank performs I/O over the runtime of the applications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_step=1 # in seconds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Request Size Timeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "values = np.arange(0, math.ceil(job_time), time_step)\n",
    "values_int = range(len(values))\n",
    "timeline_ts = [0]*len(values_int)\n",
    "prev = 0\n",
    "for i in values_int:\n",
    "    contains = (io_ddf_read_write['tstart'] >=prev) & (io_ddf_read_write['tend'] <values[i])\n",
    "    timeline_ts[i] = io_ddf_read_write[contains]['size'].sum()/1024.0/1024.0/1024.0\n",
    "    prev = values[i]\n",
    "print(timeline_ts)\n",
    "plt.bar(values, timeline_ts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "values = np.arange(0, math.ceil(job_time), time_step)\n",
    "values_int = range(len(values))\n",
    "timeline_ts = [0]*len(values_int)\n",
    "prev = 0\n",
    "for i in values_int:\n",
    "    contains = (io_ddf_read_write['tstart'] >=prev) & (io_ddf_read_write['tend'] <values[i]) & (read_condition)\n",
    "    timeline_ts[i] = io_ddf_read_write[contains]['size'].sum()/1024.0/1024.0/1024.0\n",
    "    prev = values[i]\n",
    "print(timeline_ts)\n",
    "plt.bar(values, timeline_ts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "values = np.arange(0, math.ceil(job_time), time_step)\n",
    "values_int = range(len(values))\n",
    "timeline_ts = [0]*len(values_int)\n",
    "prev = 0\n",
    "for i in values_int:\n",
    "    contains = (io_ddf_read_write['tstart'] >=prev) & (io_ddf_read_write['tend'] <values[i]) & (write_condition)\n",
    "    timeline_ts[i] = io_ddf_read_write[contains]['size'].sum()/1024.0/1024.0/1024.0\n",
    "    prev = values[i]\n",
    "print(timeline_ts)\n",
    "plt.bar(values, timeline_ts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "Finding"
    ]
   },
   "source": [
    "The initial part of the application is spent on init. Based on code inspection this is memory allocation. Then we have I/O operations. Also, as the application has same number of ops a variatic graph depicts that the I/O performance is not consistant during the runtime. Specifically, the I/O performance can reduce 2x-10x during runtime of application."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bandwidth Timeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "values = np.arange(0, math.ceil(job_time), time_step)\n",
    "values_int = range(len(values))\n",
    "timeline_ts = [0]*len(values_int)\n",
    "prev = 0\n",
    "for i in values_int:\n",
    "    contains = (io_ddf_read_write['tstart'] >=prev) & (io_ddf_read_write['tend'] <values[i])\n",
    "    timeline_ts[i] = io_ddf_read_write[contains]['bandwidth'].mean()/1024.0\n",
    "    prev = values[i]\n",
    "print(timeline_ts)\n",
    "plt.bar(values, timeline_ts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "values = np.arange(0, math.ceil(job_time), time_step)\n",
    "values_int = range(len(values))\n",
    "timeline_ts = [0]*len(values_int)\n",
    "prev = 0\n",
    "for i in values_int:\n",
    "    contains = (io_ddf_read_write['tstart'] >=prev) & (io_ddf_read_write['tend'] <values[i]) & read_condition\n",
    "    timeline_ts[i] = io_ddf_read_write[contains]['bandwidth'].mean()/1024.0\n",
    "    prev = values[i]\n",
    "print(timeline_ts)\n",
    "plt.bar(values, timeline_ts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "values = np.arange(0, math.ceil(job_time), time_step)\n",
    "values_int = range(len(values))\n",
    "timeline_ts = [0]*len(values_int)\n",
    "prev = 0\n",
    "for i in values_int:\n",
    "    contains = (io_ddf_read_write['tstart'] >=prev) & (io_ddf_read_write['tend'] <values[i]) & write_condition\n",
    "    timeline_ts[i] = io_ddf_read_write[contains]['bandwidth'].mean()/1024.0\n",
    "    prev = values[i]\n",
    "print(timeline_ts)\n",
    "plt.bar(values, timeline_ts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This supports our previous observation as the bandwidth is the measure of I/O performance of the application."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bandwidth Timeline Per Rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "selected_rank = [0, 4, 8, 12, 20]\n",
    "for rank in selected_rank:\n",
    "    io_df_rank = io_ddf_read_write[io_ddf_read_write['rank'] == rank]\n",
    "    values = np.arange(0, math.ceil(job_time), time_step)\n",
    "    values_int = range(len(values))\n",
    "    timeline_ts = [0]*len(values_int)\n",
    "    prev = 0\n",
    "    for i in values_int:\n",
    "        contains = (io_df_rank['tstart'] >=prev) & (io_df_rank['tend'] <values[i])\n",
    "        timeline_ts[i] = io_df_rank[contains]['bandwidth'].mean()\n",
    "        prev = values[i]\n",
    "    plt.bar(values, timeline_ts)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "Finding"
    ]
   },
   "source": [
    "As each of the processes in the job perform the same work, the variation in bandwidth means we can optimize the workflow with buffering and caching to reduce the stress and network congestions on the PFS."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Request Size Timeline per Rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "selected_rank = [0, 4, 8, 12, 20]\n",
    "for rank in selected_rank:\n",
    "    io_df_rank = io_ddf_read_write[io_ddf_read_write['rank'] == rank]\n",
    "    values = np.arange(0, math.ceil(job_time), time_step)\n",
    "    values_int = range(len(values))\n",
    "    timeline_ts = [0]*len(values_int)\n",
    "    prev = 0\n",
    "    for i in values_int:\n",
    "        contains = (io_df_rank['tstart'] >=prev) & (io_df_rank['tend'] <values[i])\n",
    "        timeline_ts[i] = io_df_rank[contains]['size'].sum()\n",
    "        prev = values[i]\n",
    "    plt.bar(values, timeline_ts)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Flow\n",
    "We plot how different ranks in the job are accessing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dependency_graph = {}\n",
    "for filename in all_filenames:\n",
    "    dependency_graph[filename]={}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import graphviz\n",
    "dot = graphviz.Digraph(\"rank0\",format='pdf')\n",
    "for rank in [0]:\n",
    "    dot.node(str(rank))\n",
    "    io_access_rank = io_ddf[io_ddf['rank'] == rank].groupby(['filename','func_id'])['func_id'].count().compute()\n",
    "    #print(io_access_rank)\n",
    "    for index,item in io_access_rank.iteritems():\n",
    "        filename = index[0]\n",
    "        func_id = \"{}-{}\".format(rank,index[1])\n",
    "        count = item\n",
    "        #print(filename,func_id,count)\n",
    "        if \"/p/gpfs1\" in filename:\n",
    "            #if filename not in dependency_graph:\n",
    "            #    dependency_graph[filename]={}\n",
    "            #if rank not in dependency_graph[filename]:\n",
    "            #    dependency_graph[filename][rank] = {}\n",
    "            #if index[1] not in dependency_graph[filename][rank]:\n",
    "            #    dependency_graph[filename][rank][index[1]] = 0\n",
    "            #dependency_graph[filename][rank][index[1]] += count\n",
    "            dot.node(func_id)\n",
    "            dot.node(filename)\n",
    "            dot.edge(str(rank),func_id)\n",
    "            if \"read\" in func_id:\n",
    "                dot.edge(filename, func_id, label=str(count))\n",
    "            else:\n",
    "                dot.edge(func_id,filename, label=str(count))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dot.source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!dot -Tpdf rank0.dot > rank0.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "Finding"
    ]
   },
   "source": [
    "Through this we can hypothesize that each process is independently reading and writing data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find file which are independent or shared\n",
    "- Make all thread id start from 0 and unique across ranks\n",
    "- Group by filename nunique thread_id\n",
    "- Find all filename with nunique > 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thread_index_series = io_ddf.groupby(['rank', 'thread_id'])['thread_id'].nunique().cumsum().compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "io_ddf['thread_index'] = 0\n",
    "for index,item in thread_index_series.iteritems():\n",
    "    condition = (io_ddf['rank'] == index[0]) & (io_ddf['thread_id'] == index[1])\n",
    "    io_ddf['thread_index'] = io_ddf['thread_index'].mask(condition , item - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threads = io_ddf['thread_index'].nunique().compute()\n",
    "print(\"We have {} threads across {} ranks\".format(threads, num_ranks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename_series = io_ddf[io_ddf['filename'].str.contains('/p/gpfs1')].groupby(['filename'])['thread_index'].nunique().compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "Finding"
    ]
   },
   "outputs": [],
   "source": [
    "independent_files = filename_series[filename_series == 1]\n",
    "print(\"{} files that are accessed by application by only one rank\".format(len(independent_files)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "Finding"
    ]
   },
   "outputs": [],
   "source": [
    "shared_files = filename_series[filename_series > 1]\n",
    "print(\"{} files that are accessed by application by more than one rank\".format(len(shared_files)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "Finding"
    ]
   },
   "source": [
    "This application doesnt share files. That is it follows a File per process pattern."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate Access Pattern\n",
    "- Calculate segment_index\n",
    "  - select file_ptr based on open flag\n",
    "  - update file_ptr based on operation\n",
    "  - do a cum_sum on file_ptr to calculate final file_ptr\n",
    "  - assign segment_index based on granularity (median transfer size)\n",
    "- isolate segment index into\n",
    "  - sequential flag if segment index is increasing\n",
    "  - consequitive flag if segment index is increasing and one after the other"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Persist the current version of I/O dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "io_ddf = io_ddf.persist()\n",
    "result = wait(io_ddf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get File operations per rank and per file from the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_ranks = [0]#range(num_ranks)##\n",
    "file_per_rank_df = [{}]*len(selected_ranks)\n",
    "for rank in selected_ranks:\n",
    "    file_per_rank_df[rank]={}\n",
    "    io_ddf_rank = io_ddf[io_ddf['rank'] == rank]\n",
    "    unique_files = io_ddf_rank['filename'].unique().compute()\n",
    "    for filename in unique_files:\n",
    "        if filename != '' and '/p/gpfs1' in filename:\n",
    "            file_per_rank_df[rank][filename] = io_ddf_rank[io_ddf_rank['filename'] == filename].compute()\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check Pattern\n",
    "We just check if seeks are used. If not it uses a sequential pattern."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for rank, file_per_rank_df_single in enumerate(file_per_rank_df):\n",
    "    for filename in file_per_rank_df_single:\n",
    "        file_df = file_per_rank_df_single[filename]\n",
    "        ops = file_df['func_id'].unique()\n",
    "        if \"seek\" not in ops:\n",
    "            print(\"file {} is acessed sequentially by rank {}\".format(filename, rank))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "Finding"
    ]
   },
   "source": [
    "There is no seek in any of the file access hence they are sequentially accessed overall."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Per File Analysis\n",
    "- Total I/O amount\n",
    "- Total I/O time (average per process)\n",
    "- Average Bandwidth\n",
    "- I/O Request Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_df = io_ddf_read_write\n",
    "per_file_df = io_ddf_read_write.groupby(['filename'])\n",
    "temp_df['size_sum'] = per_file_df[[\"size\"]].transform(sum)\n",
    "temp_df['size_sum'] = temp_df['size_sum'] / 1024.0 / 1024.0\n",
    "temp_df['time_sum'] = per_file_df[[\"duration\"]].transform(sum)\n",
    "temp_df['bw_sum'] = per_file_df[[\"bandwidth\"]].transform(np.mean)\n",
    "\n",
    "per_file_size = temp_df.sort_values(\"size_sum\", ascending=False)[\"size_sum\"]\n",
    "per_file_time = temp_df.sort_values(\"time_sum\", ascending=False)[\"time_sum\"]\n",
    "per_file_bw = temp_df.sort_values(\"bw_sum\", ascending=False)[\"bw_sum\"]\n",
    "print(per_file_size.head())\n",
    "print(per_file_time.head())\n",
    "print(per_file_bw.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "iopp",
   "language": "python",
   "name": "iopp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc-showtags": false,
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
